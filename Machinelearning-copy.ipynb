{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e333dc",
   "metadata": {},
   "source": [
    "## Predictive model - machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f782b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>4.930</td>\n",
       "      <td>4.710</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.900</td>\n",
       "      <td>5524711.0</td>\n",
       "      <td>4.150702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>4.880</td>\n",
       "      <td>4.730</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.830</td>\n",
       "      <td>5718988.0</td>\n",
       "      <td>4.091407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>4.790</td>\n",
       "      <td>4.580</td>\n",
       "      <td>4.79</td>\n",
       "      <td>4.710</td>\n",
       "      <td>6846296.0</td>\n",
       "      <td>3.989757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>4.645</td>\n",
       "      <td>4.290</td>\n",
       "      <td>4.56</td>\n",
       "      <td>4.350</td>\n",
       "      <td>10624829.0</td>\n",
       "      <td>3.684807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>4.460</td>\n",
       "      <td>4.260</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.440</td>\n",
       "      <td>10001706.0</td>\n",
       "      <td>3.761045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12</th>\n",
       "      <td>6.120</td>\n",
       "      <td>5.965</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.000</td>\n",
       "      <td>4434183.0</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-15</th>\n",
       "      <td>6.090</td>\n",
       "      <td>6.000</td>\n",
       "      <td>6.03</td>\n",
       "      <td>6.060</td>\n",
       "      <td>3060034.0</td>\n",
       "      <td>6.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-16</th>\n",
       "      <td>6.125</td>\n",
       "      <td>6.020</td>\n",
       "      <td>6.10</td>\n",
       "      <td>6.040</td>\n",
       "      <td>3333978.0</td>\n",
       "      <td>6.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-17</th>\n",
       "      <td>6.090</td>\n",
       "      <td>5.980</td>\n",
       "      <td>6.06</td>\n",
       "      <td>6.070</td>\n",
       "      <td>3201553.0</td>\n",
       "      <td>6.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-18</th>\n",
       "      <td>5.940</td>\n",
       "      <td>5.620</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.935</td>\n",
       "      <td>7274164.0</td>\n",
       "      <td>5.935000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1678 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             High    Low  Open  Close      Volume  Adj Close\n",
       "Date                                                        \n",
       "2016-01-03  4.930  4.710  4.71  4.900   5524711.0   4.150702\n",
       "2016-01-04  4.880  4.730  4.87  4.830   5718988.0   4.091407\n",
       "2016-01-05  4.790  4.580  4.79  4.710   6846296.0   3.989757\n",
       "2016-01-06  4.645  4.290  4.56  4.350  10624829.0   3.684807\n",
       "2016-01-07  4.460  4.260  4.26  4.440  10001706.0   3.761045\n",
       "...           ...    ...   ...    ...         ...        ...\n",
       "2022-08-12  6.120  5.965  6.09  6.000   4434183.0   6.000000\n",
       "2022-08-15  6.090  6.000  6.03  6.060   3060034.0   6.060000\n",
       "2022-08-16  6.125  6.020  6.10  6.040   3333978.0   6.040000\n",
       "2022-08-17  6.090  5.980  6.06  6.070   3201553.0   6.070000\n",
       "2022-08-18  5.940  5.620  5.82  5.935   7274164.0   5.935000\n",
       "\n",
       "[1678 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import org data\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as web\n",
    "import datetime\n",
    "start = datetime.datetime(2016,1,1)\n",
    "end = datetime.date.today()\n",
    "org = web.DataReader(\"ORG.AX\", 'yahoo', start, end)\n",
    "org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ec608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42c8bd4b",
   "metadata": {},
   "source": [
    "1.Predicting Stock price by time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41620002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "741414bb",
   "metadata": {},
   "source": [
    "2. Predicting stock price with Moving Average (MA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d4d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a95af39",
   "metadata": {},
   "source": [
    "3. LSTMs (Long Short-term Memory) for the time-series data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c3deb",
   "metadata": {},
   "source": [
    "3.1 Data set up for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "545fa48f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>4.930</td>\n",
       "      <td>4.710</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.90</td>\n",
       "      <td>5524711.0</td>\n",
       "      <td>4.150702</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>4.880</td>\n",
       "      <td>4.730</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5718988.0</td>\n",
       "      <td>4.091407</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>4.790</td>\n",
       "      <td>4.580</td>\n",
       "      <td>4.79</td>\n",
       "      <td>4.71</td>\n",
       "      <td>6846296.0</td>\n",
       "      <td>3.989757</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>4.645</td>\n",
       "      <td>4.290</td>\n",
       "      <td>4.56</td>\n",
       "      <td>4.35</td>\n",
       "      <td>10624829.0</td>\n",
       "      <td>3.684807</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>4.460</td>\n",
       "      <td>4.260</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.44</td>\n",
       "      <td>10001706.0</td>\n",
       "      <td>3.761045</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-10</th>\n",
       "      <td>4.440</td>\n",
       "      <td>4.100</td>\n",
       "      <td>4.33</td>\n",
       "      <td>4.25</td>\n",
       "      <td>9808906.0</td>\n",
       "      <td>3.600099</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11</th>\n",
       "      <td>4.230</td>\n",
       "      <td>4.025</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.05</td>\n",
       "      <td>10981544.0</td>\n",
       "      <td>3.430683</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-12</th>\n",
       "      <td>4.200</td>\n",
       "      <td>4.010</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.18</td>\n",
       "      <td>10477083.0</td>\n",
       "      <td>3.540803</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-13</th>\n",
       "      <td>4.090</td>\n",
       "      <td>3.970</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>9203850.0</td>\n",
       "      <td>3.430683</td>\n",
       "      <td>4.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-14</th>\n",
       "      <td>4.260</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.05</td>\n",
       "      <td>7640144.0</td>\n",
       "      <td>3.430683</td>\n",
       "      <td>4.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-17</th>\n",
       "      <td>3.930</td>\n",
       "      <td>3.750</td>\n",
       "      <td>3.91</td>\n",
       "      <td>3.82</td>\n",
       "      <td>9555618.0</td>\n",
       "      <td>3.235853</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-18</th>\n",
       "      <td>3.830</td>\n",
       "      <td>3.700</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.82</td>\n",
       "      <td>9232302.0</td>\n",
       "      <td>3.235853</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-19</th>\n",
       "      <td>3.790</td>\n",
       "      <td>3.440</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.46</td>\n",
       "      <td>12954787.0</td>\n",
       "      <td>2.930904</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-20</th>\n",
       "      <td>3.980</td>\n",
       "      <td>3.460</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.69</td>\n",
       "      <td>16641495.0</td>\n",
       "      <td>3.125733</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-21</th>\n",
       "      <td>3.900</td>\n",
       "      <td>3.705</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.84</td>\n",
       "      <td>10096220.0</td>\n",
       "      <td>3.252795</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-24</th>\n",
       "      <td>4.130</td>\n",
       "      <td>3.955</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.05</td>\n",
       "      <td>9598885.0</td>\n",
       "      <td>3.430683</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-26</th>\n",
       "      <td>4.070</td>\n",
       "      <td>3.800</td>\n",
       "      <td>4.03</td>\n",
       "      <td>3.88</td>\n",
       "      <td>10130524.0</td>\n",
       "      <td>3.286679</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-27</th>\n",
       "      <td>3.940</td>\n",
       "      <td>3.850</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.87</td>\n",
       "      <td>6488444.0</td>\n",
       "      <td>3.278207</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-28</th>\n",
       "      <td>4.140</td>\n",
       "      <td>3.970</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.10</td>\n",
       "      <td>12405816.0</td>\n",
       "      <td>3.473036</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>4.340</td>\n",
       "      <td>4.105</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.21</td>\n",
       "      <td>11080170.0</td>\n",
       "      <td>3.566216</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             High    Low  Open  Close      Volume  Adj Close  label\n",
       "Date                                                               \n",
       "2016-01-03  4.930  4.710  4.71   4.90   5524711.0   4.150702   3.82\n",
       "2016-01-04  4.880  4.730  4.87   4.83   5718988.0   4.091407   3.82\n",
       "2016-01-05  4.790  4.580  4.79   4.71   6846296.0   3.989757   3.46\n",
       "2016-01-06  4.645  4.290  4.56   4.35  10624829.0   3.684807   3.69\n",
       "2016-01-07  4.460  4.260  4.26   4.44  10001706.0   3.761045   3.84\n",
       "2016-01-10  4.440  4.100  4.33   4.25   9808906.0   3.600099   4.05\n",
       "2016-01-11  4.230  4.025  4.23   4.05  10981544.0   3.430683   3.88\n",
       "2016-01-12  4.200  4.010  4.05   4.18  10477083.0   3.540803   3.87\n",
       "2016-01-13  4.090  3.970  4.00   4.05   9203850.0   3.430683   4.10\n",
       "2016-01-14  4.260  4.000  4.16   4.05   7640144.0   3.430683   4.21\n",
       "2016-01-17  3.930  3.750  3.91   3.82   9555618.0   3.235853   4.01\n",
       "2016-01-18  3.830  3.700  3.80   3.82   9232302.0   3.235853   3.80\n",
       "2016-01-19  3.790  3.440  3.77   3.46  12954787.0   2.930904   3.99\n",
       "2016-01-20  3.980  3.460  3.50   3.69  16641495.0   3.125733   4.01\n",
       "2016-01-21  3.900  3.705  3.82   3.84  10096220.0   3.252795   3.97\n",
       "2016-01-24  4.130  3.955  4.05   4.05   9598885.0   3.430683   3.82\n",
       "2016-01-26  4.070  3.800  4.03   3.88  10130524.0   3.286679   3.77\n",
       "2016-01-27  3.940  3.850  3.94   3.87   6488444.0   3.278207   3.68\n",
       "2016-01-28  4.140  3.970  3.99   4.10  12405816.0   3.473036   3.64\n",
       "2016-01-31  4.340  4.105  4.24   4.21  11080170.0   3.566216   3.70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new column called 'label' to apply shifted close price 10 days before the current day \n",
    "# set predict days as 10 days\n",
    "pre_days = 10\n",
    "org['label'] = org['Close'].shift(-pre_days)\n",
    "org.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c617f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To standardize the data\n",
    "# StandardScaler removes the mean and scales each feature/variable to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "sca_X = scaler.fit_transform(org.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c528569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664\n"
     ]
    }
   ],
   "source": [
    "# Set memory days for LSTM function\n",
    "mem_his_days = 5\n",
    "\n",
    "# Create a deque to record values as per set up memory days\n",
    "from collections import deque\n",
    "deq = deque(maxlen=mem_his_days)\n",
    "\n",
    "X = []\n",
    "for i in sca_X:\n",
    "    deq.append(list(i))\n",
    "    if len(deq) == mem_his_days:\n",
    "        X.append(list(deq))\n",
    "# The length of X so far is 1674, but need to remove 10 days from end because pre_days = 10\n",
    "X = X[:-pre_days]     \n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49273363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664\n"
     ]
    }
   ],
   "source": [
    "# Set up y values, remove memory data and pre_day data. It should have same length as X\n",
    "y = org['label'][mem_his_days-1:-pre_days]\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1f7e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# change vars to array\n",
    "X = np.array(X)\n",
    "Y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.2 NN built up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf53de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split arrays into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ad17f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow was installed for set up LSTM model learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "model = Sequential()\n",
    "# set up 10 nerual units, relevant inputs and other features, create 3 layers of NNs \n",
    "model.add(LSTM(10, input_shape=X.shape[1:],activation='relu',return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(10,activation='relu',return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(10,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1))\n",
    "# set loss as meansquarederror, metrics as meanabsolutepercentageerror for now...\n",
    "model.compile(optimizer = 'adam', loss='mse',metrics=['mape'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f8aac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 2s 9ms/step - loss: 44.0404 - mape: 98.3317 - val_loss: 40.1313 - val_mape: 95.1620\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 28.5089 - mape: 74.3838 - val_loss: 17.3498 - val_mape: 53.6160\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 13.5162 - mape: 45.6720 - val_loss: 5.1153 - val_mape: 25.7233\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 6.0266 - mape: 29.3198 - val_loss: 1.5191 - val_mape: 13.8164\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 4.1365 - mape: 24.2663 - val_loss: 0.9592 - val_mape: 11.4879\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 3.9393 - mape: 22.7990 - val_loss: 0.8673 - val_mape: 11.1020\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 3.1602 - mape: 20.9484 - val_loss: 0.6213 - val_mape: 9.0394\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 3.1170 - mape: 20.2548 - val_loss: 0.5134 - val_mape: 8.0163\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.7106 - mape: 19.1540 - val_loss: 0.4524 - val_mape: 7.6310\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.5797 - mape: 19.3280 - val_loss: 0.3978 - val_mape: 7.2034\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.4483 - mape: 18.3131 - val_loss: 0.5031 - val_mape: 8.7329\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.3647 - mape: 18.2814 - val_loss: 0.3598 - val_mape: 7.1504\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.2603 - mape: 17.8533 - val_loss: 0.3618 - val_mape: 7.2801\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.2112 - mape: 17.5408 - val_loss: 0.4148 - val_mape: 7.8655\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.1524 - mape: 17.3687 - val_loss: 0.4192 - val_mape: 8.1029\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.0716 - mape: 17.0534 - val_loss: 0.2792 - val_mape: 6.3503\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.1187 - mape: 17.2154 - val_loss: 0.3356 - val_mape: 7.1247\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.9951 - mape: 17.0342 - val_loss: 0.2771 - val_mape: 6.3824\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.9117 - mape: 16.3342 - val_loss: 0.3570 - val_mape: 7.3353\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.0118 - mape: 16.7621 - val_loss: 0.4247 - val_mape: 8.0968\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.0695 - mape: 16.9069 - val_loss: 0.3106 - val_mape: 6.8872\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.9860 - mape: 16.5599 - val_loss: 0.3037 - val_mape: 6.7933\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.8994 - mape: 16.6502 - val_loss: 0.4121 - val_mape: 8.0621\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.9061 - mape: 16.2407 - val_loss: 0.3181 - val_mape: 6.9970\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.8352 - mape: 15.6429 - val_loss: 0.3659 - val_mape: 7.6636\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.7698 - mape: 16.0984 - val_loss: 0.2796 - val_mape: 6.5809\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.6966 - mape: 15.2725 - val_loss: 0.3723 - val_mape: 7.6773\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.7352 - mape: 15.8261 - val_loss: 0.2449 - val_mape: 6.2091\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.6793 - mape: 15.5766 - val_loss: 0.2905 - val_mape: 6.7010\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.4849 - mape: 14.4976 - val_loss: 0.2504 - val_mape: 6.1537\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.5484 - mape: 14.7501 - val_loss: 0.2952 - val_mape: 6.7561\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.6179 - mape: 14.9875 - val_loss: 0.3240 - val_mape: 6.9049\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.4562 - mape: 14.1460 - val_loss: 0.2977 - val_mape: 6.7965\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.4651 - mape: 14.5319 - val_loss: 0.2934 - val_mape: 6.7326\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.4372 - mape: 14.0749 - val_loss: 0.2994 - val_mape: 6.8171\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.4141 - mape: 13.7776 - val_loss: 0.3187 - val_mape: 7.0719\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.4723 - mape: 14.1704 - val_loss: 0.2594 - val_mape: 6.3086\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.4207 - mape: 14.1361 - val_loss: 0.3198 - val_mape: 7.0038\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.3565 - mape: 13.8396 - val_loss: 0.2808 - val_mape: 6.5892\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.3946 - mape: 14.0301 - val_loss: 0.2579 - val_mape: 6.2510\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.4055 - mape: 14.1850 - val_loss: 0.2589 - val_mape: 6.2171\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.3148 - mape: 13.6988 - val_loss: 0.2579 - val_mape: 6.2319\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.2167 - mape: 12.7972 - val_loss: 0.2718 - val_mape: 6.4638\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.2859 - mape: 13.4098 - val_loss: 0.3241 - val_mape: 7.1408\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.3284 - mape: 13.4518 - val_loss: 0.2388 - val_mape: 6.2282\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.2839 - mape: 13.3330 - val_loss: 0.2754 - val_mape: 6.5278\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.2677 - mape: 13.4239 - val_loss: 0.2326 - val_mape: 6.1185\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.2902 - mape: 13.3489 - val_loss: 0.2388 - val_mape: 6.0183\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.3126 - mape: 13.5030 - val_loss: 0.2543 - val_mape: 6.2614\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.3348 - mape: 13.4132 - val_loss: 0.2849 - val_mape: 6.6363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2be7da853d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model 1\n",
    "model.fit(X_train,y_train,batch_size=32,epochs=50,validation_data=(X_test,y_test))\n",
    "# batch_size=32,epochs=50   -----> loss: 1.3348 - mape: 13.4132 - val_loss: 0.2849 - val_mape: 6.6363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9895bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.2686 - mape: 13.4181 - val_loss: 0.2406 - val_mape: 6.0239\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.1886 - mape: 12.6868 - val_loss: 0.2293 - val_mape: 5.8819\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.2033 - mape: 13.2060 - val_loss: 0.3164 - val_mape: 6.9519\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.1834 - mape: 12.8273 - val_loss: 0.2795 - val_mape: 6.5053\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.2321 - mape: 13.2471 - val_loss: 0.2463 - val_mape: 6.1621\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.1126 - mape: 12.4286 - val_loss: 0.2849 - val_mape: 6.5643\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.1403 - mape: 12.4608 - val_loss: 0.3242 - val_mape: 7.0207\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.1080 - mape: 12.4854 - val_loss: 0.2278 - val_mape: 5.8526\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.2125 - mape: 12.6947 - val_loss: 0.2249 - val_mape: 5.8080\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.1414 - mape: 12.6336 - val_loss: 0.2962 - val_mape: 6.7104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2be093afd60>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model 2\n",
    "model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_test,y_test))\n",
    "# batch_size=32,epochs=10   -----> loss: 1.1414 - mape: 12.6336 - val_loss: 0.2962 - val_mape: 6.7104, not showing big difference by changing epochs values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86e6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6466c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
